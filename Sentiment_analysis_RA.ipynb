{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up, data, dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Davide/Documents/University/RA/text_analysis/sentiment_analysis-master'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "#Set working directory \n",
    "#os.chdir('/Users/Davide/Documents/University/RA/text_analysis')\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraped data\n",
    "headlines_df = pd.read_csv('express.csv', index_col = 'date')\n",
    "\n",
    "daily_mail_df = pd.read_excel('daily_mail_after_filters.xlsx', index_col = 'date')\n",
    "evening_standard_df = pd.read_excel('evening_standard_after_filters.xlsx', index_col = 'date')\n",
    "express_df = pd.read_excel('express_after_filters.xlsx', index_col = 'date')\n",
    "guardian_df = pd.read_excel('guardian_after_filters.xlsx', index_col = 'date')\n",
    "independent_df = pd.read_excel('independent_after_filters.xlsx', index_col = 'date')\n",
    "times_df = pd.read_excel('times_after_filters.xlsx', index_col = 'date')\n",
    "\n",
    "#Concatenating all articles together\n",
    "newspapers = [daily_mail_df, evening_standard_df, express_df, guardian_df, independent_df, times_df]\n",
    "articles_df = pd.concat(newspapers)\n",
    "\n",
    "#labeled data\n",
    "bbc_df = pd.read_csv('News_dataset.csv', sep=';')\n",
    "kaggle_df = pd.read_json (r'/Users/Davide/Documents/University/RA/text_analysis/sentiment_analysis-master/News_Category_Dataset_v2.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>author</th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Two-thirds of laws in Britain over the past tw...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Tamara Cohen, Political Correspondent for the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Study found 64.7 per cent of the laws made bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Now Europe wants to ban your halogen light bul...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Sean Poulter, Consumer Affairs Editor For The ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Follows ban of incandescent bulbs in bid to cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Scott Walker admits flip-flop on illegal immig...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Associated Press</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wisconsin governor once favored plan whereby u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Don't blame immigrants for ills of society, sa...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Matt Chorley, Political Editor for MailOnline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Letter to 500,000 parishes warns of blaming im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-2015</th>\n",
       "      <td>Landmark EU ruling to cut plastic bag use by 8...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Sean Poulter for the Daily Mail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EU decision is a victory for Daily Mail reader...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2017</th>\n",
       "      <td>Warning that future depends on better infrastr...</td>\n",
       "      <td>The Times</td>\n",
       "      <td>Peter O’Dwyer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ireland’s leading construction lobby group h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24-3-2017</th>\n",
       "      <td>Your five-minute digest</td>\n",
       "      <td>The Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 Gambling bosses are at war over fixed-odds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27-3-2017</th>\n",
       "      <td>Your five-minute digest</td>\n",
       "      <td>The Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TodayThe Bank of England’s financial policy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30-3-2017</th>\n",
       "      <td>Your five-minute digest</td>\n",
       "      <td>The Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 Kim Mears, a managing director of BT Openr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Your five-minute digest</td>\n",
       "      <td>The Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 Banks are denying mortgages to European Un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53992 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    headline   newspaper  \\\n",
       "date                                                                       \n",
       "1-3-2015   Two-thirds of laws in Britain over the past tw...  Daily Mail   \n",
       "1-3-2015   Now Europe wants to ban your halogen light bul...  Daily Mail   \n",
       "1-3-2015   Scott Walker admits flip-flop on illegal immig...  Daily Mail   \n",
       "1-3-2015   Don't blame immigrants for ills of society, sa...  Daily Mail   \n",
       "2-3-2015   Landmark EU ruling to cut plastic bag use by 8...  Daily Mail   \n",
       "...                                                      ...         ...   \n",
       "1-3-2017   Warning that future depends on better infrastr...   The Times   \n",
       "24-3-2017                            Your five-minute digest   The Times   \n",
       "27-3-2017                            Your five-minute digest   The Times   \n",
       "30-3-2017                            Your five-minute digest   The Times   \n",
       "31-3-2017                            Your five-minute digest   The Times   \n",
       "\n",
       "                                                      author page  \\\n",
       "date                                                                \n",
       "1-3-2015   Tamara Cohen, Political Correspondent for the ...  NaN   \n",
       "1-3-2015   Sean Poulter, Consumer Affairs Editor For The ...  NaN   \n",
       "1-3-2015                                   Associated Press   NaN   \n",
       "1-3-2015     Matt Chorley, Political Editor for MailOnline    NaN   \n",
       "2-3-2015                    Sean Poulter for the Daily Mail   NaN   \n",
       "...                                                      ...  ...   \n",
       "1-3-2017                                       Peter O’Dwyer  NaN   \n",
       "24-3-2017                                                NaN  NaN   \n",
       "27-3-2017                                                NaN  NaN   \n",
       "30-3-2017                                                NaN  NaN   \n",
       "31-3-2017                                                NaN  NaN   \n",
       "\n",
       "                                                        text  \n",
       "date                                                          \n",
       "1-3-2015   Study found 64.7 per cent of the laws made bet...  \n",
       "1-3-2015   Follows ban of incandescent bulbs in bid to cu...  \n",
       "1-3-2015   Wisconsin governor once favored plan whereby u...  \n",
       "1-3-2015   Letter to 500,000 parishes warns of blaming im...  \n",
       "2-3-2015   EU decision is a victory for Daily Mail reader...  \n",
       "...                                                      ...  \n",
       "1-3-2017     Ireland’s leading construction lobby group h...  \n",
       "24-3-2017    1 Gambling bosses are at war over fixed-odds...  \n",
       "27-3-2017    TodayThe Bank of England’s financial policy ...  \n",
       "30-3-2017    1 Kim Mears, a managing director of BT Openr...  \n",
       "31-3-2017    1 Banks are denying mortgages to European Un...  \n",
       "\n",
       "[53992 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Prince Andrew 'frozen out by Charles over dama...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>EXCLUSIVE: Migrants to put Britain's populatio...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>EXCLUSIVE: Jihadi John exposed by web error: K...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Missing Becky Watts: Two arrested as family ad...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>We can all benefit from a positive approach to...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Man United player: I was worried after manager...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Mark Lawrenson: My biggest worry about Liverpool</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>La Liga ace reveals messages received from Man...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Real Madrid News: James Rodriguez wants to joi...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Star midfielder ruled out of Merseyside derby:...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219422 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    headline newspaper\n",
       "date                                                                  \n",
       "1-3-2015   Prince Andrew 'frozen out by Charles over dama...   Express\n",
       "1-3-2015   EXCLUSIVE: Migrants to put Britain's populatio...   Express\n",
       "1-3-2015   EXCLUSIVE: Jihadi John exposed by web error: K...   Express\n",
       "1-3-2015   Missing Becky Watts: Two arrested as family ad...   Express\n",
       "1-3-2015   We can all benefit from a positive approach to...   Express\n",
       "...                                                      ...       ...\n",
       "31-3-2017  Man United player: I was worried after manager...   Express\n",
       "31-3-2017   Mark Lawrenson: My biggest worry about Liverpool   Express\n",
       "31-3-2017  La Liga ace reveals messages received from Man...   Express\n",
       "31-3-2017  Real Madrid News: James Rodriguez wants to joi...   Express\n",
       "31-3-2017  Star midfielder ruled out of Merseyside derby:...   Express\n",
       "\n",
       "[219422 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complete_Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "      <td>001.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "      <td>002.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "      <td>003.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "      <td>004.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "      <td>005.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>397.txt</td>\n",
       "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
       "      <td>tech</td>\n",
       "      <td>397.txt-tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>398.txt</td>\n",
       "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
       "      <td>tech</td>\n",
       "      <td>398.txt-tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>399.txt</td>\n",
       "      <td>Be careful how you code\\n\\nA new European dire...</td>\n",
       "      <td>tech</td>\n",
       "      <td>399.txt-tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt-tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming\\n\\nOnline rol...</td>\n",
       "      <td>tech</td>\n",
       "      <td>401.txt-tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_Name                                            Content  Category  \\\n",
       "0      001.txt  Ad sales boost Time Warner profit\\n\\nQuarterly...  business   \n",
       "1      002.txt  Dollar gains on Greenspan speech\\n\\nThe dollar...  business   \n",
       "2      003.txt  Yukos unit buyer faces loan claim\\n\\nThe owner...  business   \n",
       "3      004.txt  High fuel prices hit BA's profits\\n\\nBritish A...  business   \n",
       "4      005.txt  Pernod takeover talk lifts Domecq\\n\\nShares in...  business   \n",
       "...        ...                                                ...       ...   \n",
       "2220   397.txt  BT program to beat dialler scams\\n\\nBT is intr...      tech   \n",
       "2221   398.txt  Spam e-mails tempt net shoppers\\n\\nComputer us...      tech   \n",
       "2222   399.txt  Be careful how you code\\n\\nA new European dire...      tech   \n",
       "2223   400.txt  US cyber security chief resigns\\n\\nThe man mak...      tech   \n",
       "2224   401.txt  Losing yourself in online gaming\\n\\nOnline rol...      tech   \n",
       "\n",
       "     Complete_Filename  \n",
       "0     001.txt-business  \n",
       "1     002.txt-business  \n",
       "2     003.txt-business  \n",
       "3     004.txt-business  \n",
       "4     005.txt-business  \n",
       "...                ...  \n",
       "2220      397.txt-tech  \n",
       "2221      398.txt-tech  \n",
       "2222      399.txt-tech  \n",
       "2223      400.txt-tech  \n",
       "2224      401.txt-tech  \n",
       "\n",
       "[2225 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slice the df to have only Green and environment articles\n",
    "green_df = kaggle_df[kaggle_df.category == 'GREEN']\n",
    "env_df = kaggle_df[kaggle_df.category == 'ENVIRONMENT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning\n",
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def cleaning(df, column_str):\n",
    "    '''function to clean a text within a column of a dataframe. The column name has to be a string'''\n",
    "    #Special character\n",
    "    df[column_str] = df[column_str].str.replace(\"\\r\", \" \")\n",
    "    df[column_str] = df[column_str].str.replace(\"\\n\", \" \")\n",
    "    df[column_str] = df[column_str].str.replace(\"    \", \" \")\n",
    "    df[column_str] = df[column_str].str.replace('\"', '')\n",
    "\n",
    "    #Lowercase\n",
    "    df[column_str] = df[column_str].str.lower()\n",
    "\n",
    "    #Punctuation\n",
    "    punctuation_signs = list(\"?:!.,;-\")\n",
    "\n",
    "    for punct_sign in punctuation_signs:\n",
    "        df[column_str] = df[column_str].str.replace(punct_sign, '')\n",
    "\n",
    "    #Possessive pronouns\n",
    "    df[column_str] = df[column_str].str.replace(\"'s\", \"\")\n",
    "    df[column_str] = df[column_str].str.replace(\"'\", \"\")\n",
    "\n",
    "    #Lemmatization\n",
    "    # Saving the lemmatizer into an object\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    nrows = len(df)\n",
    "    lemmatized_text_list = []\n",
    "\n",
    "    for row in range(0, nrows):\n",
    "\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "\n",
    "        # Save the text and its words into an object\n",
    "        text = df.iloc[row][column_str]\n",
    "        text_words = text.split(\" \")\n",
    "\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "\n",
    "    df[column_str] = lemmatized_text_list\n",
    "\n",
    "    #Stopwords\n",
    "    # Downloading the stop words list\n",
    "    nltk.download('stopwords')\n",
    "    # Loading the stop words in english\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    df[column_str] = df[column_str]\n",
    "\n",
    "    for stop_word in stop_words:\n",
    "\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        df[column_str] = df[column_str].str.replace(regex_stopword, '')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "green_df = cleaning(green_df, 'headline')\n",
    "bbc_df = cleaning(bbc_df, 'Content')\n",
    "env_df = cleaning(env_df, 'headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df = bbc_df[bbc_df.Category == 'business']\n",
    "pol_df = bbc_df[bbc_df.Category == 'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most commonly used words extractor\n",
    "def keywords_extractor(df, headline_str, n):\n",
    "    '''Return the most commonly used words from a dataframe with a column containing text.\n",
    "    Arguments are dataframe, the name of the column in string format, and the number n of keywords needed'''\n",
    "    df_words = Counter()\n",
    "    df[headline_str].str.split().apply(df_words.update)\n",
    "    df_words = df_words.most_common(20)\n",
    "    df_words_lst = []\n",
    "    for tup in df_words:\n",
    "        df_words_lst.append(tup[0])\n",
    "    return df_words_lst\n",
    "\n",
    "#Keywords list creation\n",
    "biz_keywords = keywords_extractor(biz_df, 'Content', n = 20)\n",
    "pol_keywords = keywords_extractor(pol_df, 'Content', n = 20)\n",
    "green_keywords = keywords_extractor(green_df, 'headline', n = 20)\n",
    "env_keywords = keywords_extractor(env_df, 'headline', n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'us', 'year', 'company', 'mr', 'firm', 'market', 'would', 'bank', 'rise', 'also', 'new', 'price', 'share', 'growth', 'last', 'economy', 'make', 'government', 'sales']\n"
     ]
    }
   ],
   "source": [
    "print(biz_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'mr', 'would', 'labour', 'government', 'party', 'people', 'blair', 'election', 'minister', 'plan', 'make', 'also', 'new', 'tell', 'could', 'brown', 'go', 'tax', 'lord']\n"
     ]
    }
   ],
   "source": [
    "print(pol_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['climate', 'kill', 'california', 'day', 'dog', 'oil', 'week', 'find', 'extreme', 'energy', 'get', 'make', 'save', 'us', 'green', 'change', 'trump', 'animal', 'picture', 'world', 'water', 'could', 'take', 'say', 'photos', 'watch', 'weather', '2012', 'baby', 'hurricane', 'new', 'climate change', 'paris agreement', 'environment', 'global warming', 'unfccc']\n"
     ]
    }
   ],
   "source": [
    "#Combining green and env keywords\n",
    "climate_keywords = []\n",
    "climate_keywords.extend(green_keywords)\n",
    "climate_keywords.extend(env_keywords)\n",
    "\n",
    "#Eliminating duplicates\n",
    "climate_keywords = list(set(climate_keywords))\n",
    "\n",
    "#Removing not useful keywords\n",
    "climate_keywords.remove('(photos)')\n",
    "climate_keywords.remove('(video)')\n",
    "\n",
    "#Including some additional important words\n",
    "climate_keywords.extend([\"climate change\", \"paris agreement\", \"environment\", \"global warming\", \"unfccc\"])\n",
    "\n",
    "print(climate_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "british_keywords = [\"brexit\", \"european union\", \" eu \", \"british\", \"british identity\", \"british passport\",\n",
    "                    \"british culture\", \"british heritage\", \"british goods\", \"british products\",\"british manufacturing\",\n",
    "                    \"made in britain\"]\n",
    "immigration_keywords = ['migrant', 'refugee', 'immigrant', 'asylum', 'Calais']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the functions that need to be used \n",
    "\n",
    "\n",
    "def topic_classifier(string, keywords_tuple, name_class_tuple):\n",
    "    '''Classifies a list of strings according to keywords lists presented inside a tuple.\n",
    "    Also requires the names of the classes in string from in a second tuple'''\n",
    "    string = string.lower()\n",
    "    if len(keywords_tuple) == len(name_class_tuple) and len(keywords_tuple) > 0:\n",
    "        no_of_classes = len(keywords_tuple)\n",
    "        counts = []\n",
    "        count = 0\n",
    "        while no_of_classes > 0:\n",
    "            no_of_classes -= 1\n",
    "            for keyword in keywords_tuple[no_of_classes]:\n",
    "                if keyword in string:\n",
    "                    count += 1\n",
    "            counts.insert(0, count)\n",
    "            count = 0\n",
    "    else:\n",
    "        return \"List of keywords do not match list of classes or the list of keywords is empty\"\n",
    "        \n",
    "    if max(counts) > 0:\n",
    "        max_list = max(counts)\n",
    "        values = np.array(counts)\n",
    "        ii = np.where(values==max_list)[0]\n",
    "        results_temp = [name_class_tuple[index] for index in ii]\n",
    "        sep = \",\"\n",
    "        results = sep.join(results_temp)\n",
    "        return results\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "\n",
    "# def headline_classifier(string, keyword_lst_class_1, keyword_lst_class_2, name_class_1 = 'class_1', name_class_2 = 'class_2'):\n",
    "#     '''Classifies a list of strings according to keyword lists for class_1 and class_2'''\n",
    "#     string = string.lower()\n",
    "#     count_1 = 0\n",
    "#     count_2 = 0\n",
    "#     for keyword in keyword_lst_class_1:\n",
    "#         if keyword in string:\n",
    "#             count_1 =+ 1\n",
    "#     for keyword in keyword_lst_class_2:\n",
    "\n",
    "#         if keyword in string:\n",
    "#             count_2 =+ 1\n",
    "    \n",
    "#     if count_1 > 0 and count_2 > 0:\n",
    "#         return 'both'\n",
    "#     elif count_1 > 0:\n",
    "#         return name_class_1\n",
    "#     elif count_2 > 0:\n",
    "#         return name_class_2\n",
    "#     else:\n",
    "#         return 'none'\n",
    "    \n",
    "def headline_classification(data, keywords_tuple, name_class_tuple):\n",
    "    n_row, n_col = data.shape\n",
    "    data_lst = list(data['headline'].values)\n",
    "\n",
    "    #Headline classification\n",
    "    classification_lst = []\n",
    "    for i in range(n_row):\n",
    "        data_i = str(data_lst[i]) #Include this becasue of potential nan values\n",
    "        classification_lst.append(topic_classifier(data_i, keywords_tuple, name_class_tuple))\n",
    "    return classification_lst\n",
    "\n",
    "def text_classification(data, keywords_tuple, name_class_tuple):\n",
    "    n_row, n_col = data.shape\n",
    "    data_lst = list(data['text'].values)\n",
    "\n",
    "    #Text classification\n",
    "    classification_lst = []\n",
    "    for i in range(n_row):\n",
    "        data_i = str(data_lst[i]) #Include this becasue of potential nan values\n",
    "        classification_lst.append(topic_classifier(data_i, keywords_tuple, name_class_tuple))\n",
    "    return classification_lst\n",
    "\n",
    "def sentiment_analysis (df, column_str):\n",
    "    '''function needs a dataframe with a string column where to perfrom sentiment analysis\n",
    "    provide the column name in string format'''\n",
    "    n_row, n_col = df.shape\n",
    "    headlines_lst = list(df[column_str].values)\n",
    "    #Empty list to add the polarity score\n",
    "    polarity_lst = []\n",
    "    subjectivity_lst = []\n",
    "\n",
    "    #Headline sentiment\n",
    "    for i in range(n_row):\n",
    "        headline_i = str(headlines_lst[i]) #Include this becasue of potential nan values\n",
    "        blob_headline_i = TextBlob(headline_i) #transforming string into textblob\n",
    "        polarity_lst.append(blob_headline_i.sentiment.polarity)\n",
    "        subjectivity_lst.append(blob_headline_i.sentiment.subjectivity)\n",
    "\n",
    "    #Adding polarity and subjectivity scores to the headlines dataframe\n",
    "    df['polarity '+ column_str] = polarity_lst\n",
    "    df['subjectivity '+ column_str] = subjectivity_lst\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Climate                                         3263\n",
       "Business                                        1613\n",
       "Politics                                        1598\n",
       "Climate,Business                                 772\n",
       "Business,Politics                                520\n",
       "Climate,Politics                                 487\n",
       "Climate,Business,Politics                        406\n",
       "Immigration                                      177\n",
       "Brexit                                           126\n",
       "Climate,Immigration                              118\n",
       "Brexit,Climate,Business                           74\n",
       "Brexit,Climate,Business,Politics                  72\n",
       "Brexit,Climate                                    69\n",
       "Climate,Business,Politics,Immigration             69\n",
       "Business,Immigration                              63\n",
       "Climate,Business,Immigration                      61\n",
       "Brexit,Politics                                   53\n",
       "Brexit,Business                                   46\n",
       "Climate,Politics,Immigration                      30\n",
       "Brexit,Business,Politics                          29\n",
       "Brexit,Climate,Politics                           29\n",
       "Politics,Immigration                              24\n",
       "Business,Politics,Immigration                     24\n",
       "Brexit,Climate,Immigration                        11\n",
       "Brexit,Climate,Business,Immigration                8\n",
       "Brexit,Climate,Business,Politics,Immigration       7\n",
       "Other                                              7\n",
       "Brexit,Business,Politics,Immigration               6\n",
       "Brexit,Climate,Politics,Immigration                3\n",
       "Brexit,Immigration                                 3\n",
       "Brexit,Business,Immigration                        2\n",
       "Brexit,Politics,Immigration                        1\n",
       "Name: text_classification, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Daily mail first classification\n",
    "key_tup = (british_keywords, climate_keywords, biz_keywords, pol_keywords, immigration_keywords)\n",
    "name_tup = ('Brexit', 'Climate', 'Business', 'Politics', 'Immigration')\n",
    "newspaper_lst = [daily_mail_df, evening_standard_df, express_df, guardian_df, independent_df, times_df, articles_df]\n",
    "\n",
    "# for i in range(len(newspaper_lst)):\n",
    "#     df_x = newspaper_lst[i]\n",
    "#     df_x['classification'] = ''\n",
    "#     class_lst = headline_classification(df_x, key_tup, name_tup)\n",
    "#     newspaper_lst[i] = df_x.assign(classification = class_lst)\n",
    "\n",
    "#headline classification\n",
    "daily_mail_df['headline_classification'] = ''\n",
    "head_class_lst = headline_classification(daily_mail_df, key_tup, name_tup)\n",
    "daily_mail_df = daily_mail_df.assign(headline_classification = head_class_lst)\n",
    "#sentiment_analysis\n",
    "sentiment_analysis(daily_mail_df, 'headline')\n",
    "\n",
    "#text classification\n",
    "daily_mail_df['text_classification'] = ''\n",
    "text_class_lst = text_classification(daily_mail_df, key_tup, name_tup)\n",
    "daily_mail_df = daily_mail_df.assign(text_classification = text_class_lst)\n",
    "#sentiment analysis\n",
    "sentiment_analysis(daily_mail_df, 'text')\n",
    "\n",
    "daily_mail_df.text_classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Climate                                         743\n",
       "Politics                                        594\n",
       "Business                                        390\n",
       "Climate,Business                                178\n",
       "Business,Politics                               178\n",
       "Climate,Business,Politics                       141\n",
       "Climate,Politics                                127\n",
       "Brexit                                           43\n",
       "Brexit,Climate                                   18\n",
       "Brexit,Climate,Business,Politics                 18\n",
       "Brexit,Politics                                  16\n",
       "Brexit,Climate,Politics                          14\n",
       "Climate,Immigration                              13\n",
       "Brexit,Business,Politics                         11\n",
       "Climate,Business,Immigration                     11\n",
       "Brexit,Climate,Business                          10\n",
       "Brexit,Business                                  10\n",
       "Immigration                                       9\n",
       "Climate,Business,Politics,Immigration             7\n",
       "Business,Politics,Immigration                     4\n",
       "Politics,Immigration                              4\n",
       "Climate,Politics,Immigration                      4\n",
       "Business,Immigration                              3\n",
       "Brexit,Climate,Business,Politics,Immigration      2\n",
       "Brexit,Climate,Politics,Immigration               1\n",
       "Other                                             1\n",
       "Name: text_classification, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evening Standard first classification\n",
    "#headline classification\n",
    "evening_standard_df['headline_classification'] = ''\n",
    "head_class_lst = headline_classification(evening_standard_df, key_tup, name_tup)\n",
    "evening_standard_df = evening_standard_df.assign(headline_classification = head_class_lst)\n",
    "#sentiment_analysis\n",
    "sentiment_analysis(evening_standard_df, 'headline')\n",
    "\n",
    "#text classification\n",
    "evening_standard_df['text_classification'] = ''\n",
    "text_class_lst = text_classification(evening_standard_df, key_tup, name_tup)\n",
    "evening_standard_df = evening_standard_df.assign(text_classification = text_class_lst)\n",
    "#sentiment analysis\n",
    "sentiment_analysis(evening_standard_df, 'text')\n",
    "\n",
    "evening_standard_df.text_classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                                        4430\n",
       "Climate                                         3230\n",
       "Business                                        2688\n",
       "Business,Politics                               1306\n",
       "Climate,Business                                 893\n",
       "Climate,Politics                                 696\n",
       "Climate,Business,Politics                        631\n",
       "Brexit                                           398\n",
       "Immigration                                      210\n",
       "Brexit,Climate,Business,Politics                 184\n",
       "Brexit,Climate                                   159\n",
       "Brexit,Politics                                  149\n",
       "Brexit,Climate,Business                          146\n",
       "Brexit,Business                                  144\n",
       "Brexit,Business,Politics                         144\n",
       "Climate,Immigration                              135\n",
       "Climate,Business,Immigration                     125\n",
       "Brexit,Climate,Politics                           95\n",
       "Climate,Business,Politics,Immigration             93\n",
       "Business,Immigration                              72\n",
       "Business,Politics,Immigration                     46\n",
       "Politics,Immigration                              45\n",
       "Climate,Politics,Immigration                      39\n",
       "Brexit,Climate,Business,Immigration               13\n",
       "Brexit,Climate,Business,Politics,Immigration      12\n",
       "Brexit,Climate,Immigration                        12\n",
       "Brexit,Business,Immigration                        9\n",
       "Brexit,Immigration                                 9\n",
       "Other                                              8\n",
       "Brexit,Climate,Politics,Immigration                8\n",
       "Brexit,Politics,Immigration                        5\n",
       "Brexit,Business,Politics,Immigration               4\n",
       "Name: text_classification, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Express fist classification\n",
    "#headline classification\n",
    "express_df['headline_classification'] = ''\n",
    "head_class_lst = headline_classification(express_df, key_tup, name_tup)\n",
    "express_df = express_df.assign(headline_classification = head_class_lst)\n",
    "#sentiment_analysis\n",
    "sentiment_analysis(express_df, 'headline')\n",
    "\n",
    "#text classification\n",
    "express_df['text_classification'] = ''\n",
    "text_class_lst = text_classification(express_df, key_tup, name_tup)\n",
    "express_df = express_df.assign(text_classification = text_class_lst)\n",
    "#sentiment analysis\n",
    "sentiment_analysis(express_df, 'text')\n",
    "\n",
    "express_df.text_classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Climate                                  277\n",
       "Politics                                 194\n",
       "Business                                 189\n",
       "Climate,Business                          70\n",
       "Climate,Politics                          51\n",
       "Business,Politics                         51\n",
       "Climate,Business,Politics                 34\n",
       "Brexit                                    18\n",
       "Brexit,Climate,Business,Politics          16\n",
       "Brexit,Climate,Business                   15\n",
       "Brexit,Climate                            13\n",
       "Brexit,Climate,Politics                   11\n",
       "Brexit,Politics                            8\n",
       "Immigration                                6\n",
       "Brexit,Business                            5\n",
       "Climate,Immigration                        5\n",
       "Business,Immigration                       3\n",
       "Climate,Business,Politics,Immigration      2\n",
       "Brexit,Business,Politics                   2\n",
       "Brexit,Politics,Immigration                1\n",
       "Brexit,Climate,Business,Immigration        1\n",
       "Brexit,Immigration                         1\n",
       "Climate,Business,Immigration               1\n",
       "Climate,Politics,Immigration               1\n",
       "Politics,Immigration                       1\n",
       "Name: text_classification, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Guardian first classification\n",
    "#headline classification\n",
    "guardian_df['headline_classification'] = ''\n",
    "head_class_lst = headline_classification(guardian_df, key_tup, name_tup)\n",
    "guardian_df = guardian_df.assign(headline_classification = head_class_lst)\n",
    "#sentiment_analysis\n",
    "sentiment_analysis(guardian_df, 'headline')\n",
    "\n",
    "#text classification\n",
    "guardian_df['text_classification'] = ''\n",
    "text_class_lst = text_classification(guardian_df, key_tup, name_tup)\n",
    "guardian_df = guardian_df.assign(text_classification = text_class_lst)\n",
    "#sentiment analysis\n",
    "sentiment_analysis(guardian_df, 'text')\n",
    "\n",
    "guardian_df.text_classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Politics                                        2362\n",
       "Climate                                         2290\n",
       "Business                                        1726\n",
       "Business,Politics                                672\n",
       "Climate,Business                                 551\n",
       "Climate,Politics                                 400\n",
       "Climate,Business,Politics                        350\n",
       "Brexit                                           218\n",
       "Brexit,Politics                                   99\n",
       "Brexit,Climate,Business,Politics                  95\n",
       "Immigration                                       87\n",
       "Brexit,Climate                                    81\n",
       "Brexit,Business                                   74\n",
       "Brexit,Climate,Business                           73\n",
       "Brexit,Business,Politics                          66\n",
       "Brexit,Climate,Politics                           61\n",
       "Climate,Immigration                               57\n",
       "Climate,Business,Immigration                      37\n",
       "Business,Immigration                              32\n",
       "Climate,Business,Politics,Immigration             27\n",
       "Politics,Immigration                              24\n",
       "Other                                             20\n",
       "Business,Politics,Immigration                     19\n",
       "Climate,Politics,Immigration                      14\n",
       "Brexit,Climate,Business,Politics,Immigration       9\n",
       "Brexit,Climate,Business,Immigration                8\n",
       "Brexit,Immigration                                 8\n",
       "Brexit,Climate,Politics,Immigration                2\n",
       "Brexit,Business,Politics,Immigration               2\n",
       "Brexit,Politics,Immigration                        2\n",
       "Brexit,Climate,Immigration                         1\n",
       "Brexit,Business,Immigration                        1\n",
       "Name: text_classification, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Independent first classification\n",
    "#headline classification\n",
    "independent_df['headline_classification'] = ''\n",
    "head_class_lst = headline_classification(independent_df, key_tup, name_tup)\n",
    "independent_df = independent_df.assign(headline_classification = head_class_lst)\n",
    "#sentiment_analysis\n",
    "sentiment_analysis(independent_df, 'headline')\n",
    "\n",
    "#text classification\n",
    "independent_df['text_classification'] = ''\n",
    "text_class_lst = text_classification(independent_df, key_tup, name_tup)\n",
    "independent_df = independent_df.assign(text_classification = text_class_lst)\n",
    "#sentiment analysis\n",
    "sentiment_analysis(independent_df, 'text')\n",
    "\n",
    "independent_df.text_classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Climate                                         3963\n",
       "Business                                        3838\n",
       "Politics                                        2844\n",
       "Climate,Business                                1101\n",
       "Business,Politics                                904\n",
       "Climate,Politics                                 661\n",
       "Climate,Business,Politics                        578\n",
       "Brexit                                           224\n",
       "Brexit,Climate,Business                          132\n",
       "Immigration                                      105\n",
       "Brexit,Climate                                    99\n",
       "Brexit,Climate,Business,Politics                  97\n",
       "Brexit,Business                                   96\n",
       "Brexit,Politics                                   75\n",
       "Brexit,Business,Politics                          65\n",
       "Brexit,Climate,Politics                           49\n",
       "Climate,Immigration                               47\n",
       "Climate,Business,Politics,Immigration             41\n",
       "Climate,Business,Immigration                      41\n",
       "Business,Immigration                              37\n",
       "Climate,Politics,Immigration                      23\n",
       "Politics,Immigration                              21\n",
       "Business,Politics,Immigration                     16\n",
       "Other                                              9\n",
       "Brexit,Climate,Immigration                         7\n",
       "Brexit,Climate,Business,Immigration                5\n",
       "Brexit,Climate,Business,Politics,Immigration       5\n",
       "Brexit,Business,Immigration                        3\n",
       "Brexit,Immigration                                 2\n",
       "Brexit,Business,Politics,Immigration               1\n",
       "Name: text_classification, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Times first classification\n",
    "#headline classification\n",
    "times_df['headline_classification'] = ''\n",
    "head_class_lst = headline_classification(times_df, key_tup, name_tup)\n",
    "times_df = times_df.assign(headline_classification = head_class_lst)\n",
    "#sentiment_analysis\n",
    "sentiment_analysis(times_df, 'headline')\n",
    "\n",
    "#text classification\n",
    "times_df['text_classification'] = ''\n",
    "text_class_lst = text_classification(times_df, key_tup, name_tup)\n",
    "times_df = times_df.assign(text_classification = text_class_lst)\n",
    "#sentiment analysis\n",
    "sentiment_analysis(times_df, 'text')\n",
    "\n",
    "times_df.text_classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Climate                                         13766\n",
       "Politics                                        12022\n",
       "Business                                        10444\n",
       "Business,Politics                                3631\n",
       "Climate,Business                                 3565\n",
       "Climate,Politics                                 2422\n",
       "Climate,Business,Politics                        2140\n",
       "Brexit                                           1027\n",
       "Immigration                                       594\n",
       "Brexit,Climate,Business,Politics                  482\n",
       "Brexit,Climate,Business                           450\n",
       "Brexit,Climate                                    439\n",
       "Brexit,Politics                                   400\n",
       "Climate,Immigration                               375\n",
       "Brexit,Business                                   375\n",
       "Brexit,Business,Politics                          317\n",
       "Climate,Business,Immigration                      276\n",
       "Brexit,Climate,Politics                           259\n",
       "Climate,Business,Politics,Immigration             239\n",
       "Business,Immigration                              210\n",
       "Politics,Immigration                              119\n",
       "Climate,Politics,Immigration                      111\n",
       "Business,Politics,Immigration                     109\n",
       "Other                                              45\n",
       "Brexit,Climate,Business,Politics,Immigration       35\n",
       "Brexit,Climate,Business,Immigration                35\n",
       "Brexit,Climate,Immigration                         31\n",
       "Brexit,Immigration                                 23\n",
       "Brexit,Business,Immigration                        15\n",
       "Brexit,Climate,Politics,Immigration                14\n",
       "Brexit,Business,Politics,Immigration               13\n",
       "Brexit,Politics,Immigration                         9\n",
       "Name: text_classification, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All articles first classification\n",
    "#headline classification\n",
    "articles_df['headline_classification'] = ''\n",
    "head_class_lst = headline_classification(articles_df, key_tup, name_tup)\n",
    "articles_df = articles_df.assign(headline_classification = head_class_lst)\n",
    "#sentiment_analysis\n",
    "sentiment_analysis(articles_df, 'headline')\n",
    "\n",
    "#text classification\n",
    "articles_df['text_classification'] = ''\n",
    "text_class_lst = text_classification(articles_df, key_tup, name_tup)\n",
    "articles_df = articles_df.assign(text_classification = text_class_lst)\n",
    "#sentiment analysis\n",
    "sentiment_analysis(articles_df, 'text')\n",
    "\n",
    "articles_df.text_classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other                            411\n",
       "Politics                         374\n",
       "Business                         230\n",
       "Business,Politics                111\n",
       "Immigration                       76\n",
       "Politics,Immigration              24\n",
       "Business,Politics,Immigration      8\n",
       "Business,Immigration               8\n",
       "Name: classification, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If we want to classify Brexit terms only with biz/pol/immigration \n",
    "#Slice of only the Brexit terms\n",
    "brexit_df = daily_mail_df[daily_mail_df.classification == 'Brexit']\n",
    "\n",
    "#Further classification for Brexit articles\n",
    "key_tup2 = (biz_keywords, pol_keywords, immigration_keywords)\n",
    "name_tup2 = ('Business', 'Politics', 'Immigration')\n",
    "classification_lst = []\n",
    "\n",
    "daily_mail_brexit_class = headline_classification(brexit_df, key_tup2, name_tup2)\n",
    "brexit_df = brexit_df.assign(classification = daily_mail_brexit_class)\n",
    "brexit_df.classification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis (df, column_str):\n",
    "    '''function needs a dataframe with a string column where to perfrom sentiment analysis\n",
    "    provide the column name in string format'''\n",
    "    n_row, n_col = df.shape\n",
    "    headlines_lst = list(df[column_str].values)\n",
    "    #Empty list to add the polarity score\n",
    "    polarity_lst = []\n",
    "    subjectivity_lst = []\n",
    "\n",
    "    #Headline sentiment\n",
    "    for i in range(n_row):\n",
    "        headline_i = str(headlines_lst[i]) #Include this becasue of potential nan values\n",
    "        blob_headline_i = TextBlob(headline_i) #transforming string into textblob\n",
    "        polarity_lst.append(blob_headline_i.sentiment.polarity)\n",
    "        subjectivity_lst.append(blob_headline_i.sentiment.subjectivity)\n",
    "\n",
    "    #Adding polarity and subjectivity scores to the headlines dataframe\n",
    "    df['polarity '+ column_str] = polarity_lst\n",
    "    df['subjectivity '+ column_str] = subjectivity_lst\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>author</th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Two-thirds of laws in Britain over the past tw...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Tamara Cohen, Political Correspondent for the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Study found 64.7 per cent of the laws made bet...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Now Europe wants to ban your halogen light bul...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Sean Poulter, Consumer Affairs Editor For The ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Follows ban of incandescent bulbs in bid to cu...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Scott Walker admits flip-flop on illegal immig...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Associated Press</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wisconsin governor once favored plan whereby u...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>-0.319444</td>\n",
       "      <td>0.402778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Don't blame immigrants for ills of society, sa...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Matt Chorley, Political Editor for MailOnline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Letter to 500,000 parishes warns of blaming im...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-2015</th>\n",
       "      <td>Landmark EU ruling to cut plastic bag use by 8...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Sean Poulter for the Daily Mail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EU decision is a victory for Daily Mail reader...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>People smuggler who 'drove into the US with fo...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Mail Online Reporter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jose Emiliano Aguilar, 24, is accused of tryin...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Bank of England keeps the vegans happy with pl...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Giulia Crouch For Mailonline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The plastic note was slammed by vegans when it...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0.485227</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Women putting themselves at risk by being pros...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Rebecca Taylor For Mailonline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Victoria Bateman called the ban on prostitutio...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>EU Council President Tusk demands a Brexit dea...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Tim Sculthorpe, Deputy Political Editor For Ma...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Theresa May in standoff with EU leaders after ...</td>\n",
       "      <td>Brexit</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Japan slaughters 333 whales for meat that will...</td>\n",
       "      <td>Daily Mail</td>\n",
       "      <td>Naomi Leach For Mailonline and Afp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japanese fleets slaughtered 333 minke whales d...</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9771 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    headline   newspaper  \\\n",
       "date                                                                       \n",
       "1-3-2015   Two-thirds of laws in Britain over the past tw...  Daily Mail   \n",
       "1-3-2015   Now Europe wants to ban your halogen light bul...  Daily Mail   \n",
       "1-3-2015   Scott Walker admits flip-flop on illegal immig...  Daily Mail   \n",
       "1-3-2015   Don't blame immigrants for ills of society, sa...  Daily Mail   \n",
       "2-3-2015   Landmark EU ruling to cut plastic bag use by 8...  Daily Mail   \n",
       "...                                                      ...         ...   \n",
       "31-3-2017  People smuggler who 'drove into the US with fo...  Daily Mail   \n",
       "31-3-2017  Bank of England keeps the vegans happy with pl...  Daily Mail   \n",
       "31-3-2017  Women putting themselves at risk by being pros...  Daily Mail   \n",
       "31-3-2017  EU Council President Tusk demands a Brexit dea...  Daily Mail   \n",
       "31-3-2017  Japan slaughters 333 whales for meat that will...  Daily Mail   \n",
       "\n",
       "                                                      author  page  \\\n",
       "date                                                                 \n",
       "1-3-2015   Tamara Cohen, Political Correspondent for the ...   NaN   \n",
       "1-3-2015   Sean Poulter, Consumer Affairs Editor For The ...   NaN   \n",
       "1-3-2015                                   Associated Press    NaN   \n",
       "1-3-2015     Matt Chorley, Political Editor for MailOnline     NaN   \n",
       "2-3-2015                    Sean Poulter for the Daily Mail    NaN   \n",
       "...                                                      ...   ...   \n",
       "31-3-2017                              Mail Online Reporter    NaN   \n",
       "31-3-2017                      Giulia Crouch For Mailonline    NaN   \n",
       "31-3-2017                     Rebecca Taylor For Mailonline    NaN   \n",
       "31-3-2017  Tim Sculthorpe, Deputy Political Editor For Ma...   NaN   \n",
       "31-3-2017                Naomi Leach For Mailonline and Afp    NaN   \n",
       "\n",
       "                                                        text classification  \\\n",
       "date                                                                          \n",
       "1-3-2015   Study found 64.7 per cent of the laws made bet...        Climate   \n",
       "1-3-2015   Follows ban of incandescent bulbs in bid to cu...        Climate   \n",
       "1-3-2015   Wisconsin governor once favored plan whereby u...        Climate   \n",
       "1-3-2015   Letter to 500,000 parishes warns of blaming im...        Climate   \n",
       "2-3-2015   EU decision is a victory for Daily Mail reader...        Climate   \n",
       "...                                                      ...            ...   \n",
       "31-3-2017  Jose Emiliano Aguilar, 24, is accused of tryin...        Climate   \n",
       "31-3-2017  The plastic note was slammed by vegans when it...        Climate   \n",
       "31-3-2017  Victoria Bateman called the ban on prostitutio...        Climate   \n",
       "31-3-2017  Theresa May in standoff with EU leaders after ...         Brexit   \n",
       "31-3-2017  Japanese fleets slaughtered 333 minke whales d...        Climate   \n",
       "\n",
       "           polarity  subjectivity  \n",
       "date                               \n",
       "1-3-2015  -0.250000      0.250000  \n",
       "1-3-2015   0.175000      0.275000  \n",
       "1-3-2015  -0.319444      0.402778  \n",
       "1-3-2015   0.000000      0.100000  \n",
       "2-3-2015   0.000000      0.000000  \n",
       "...             ...           ...  \n",
       "31-3-2017  0.000000      0.500000  \n",
       "31-3-2017  0.485227      0.727273  \n",
       "31-3-2017  0.200000      0.600000  \n",
       "31-3-2017  0.000000      0.000000  \n",
       "31-3-2017  0.000000      0.000000  \n",
       "\n",
       "[9771 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(daily_mail_df, 'headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for each class\n",
    "brexit_df = headlines_df[headlines_df.classification == 'Brexit']\n",
    "climate_df = headlines_df[headlines_df.classification == 'Climate']\n",
    "both_df = headlines_df[headlines_df.classification == 'both']\n",
    "\n",
    "#\n",
    "results_df = pd.DataFrame(index = ['polarity', 'subjectivity'], columns = ['Brexit', 'Climate', 'Both'])\n",
    "results_df['Brexit'] = brexit_df.mean(axis = 0, numeric_only = True)\n",
    "results_df['Climate'] = climate_df.mean(axis = 0, numeric_only = True)\n",
    "results_df['Both'] = both_df.mean(axis = 0, numeric_only = True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_row, n_col = headlines_df.shape\n",
    "# headlines_lst = list(headlines_df['headline'].values)\n",
    "\n",
    "# #Headline classification\n",
    "# classification_lst = []\n",
    "# for i in range(n_row):\n",
    "#     headline_i = str(headlines_lst[i]) #Include this becasue of potential nan values\n",
    "#     classification_lst.append(headline_classifier(headline_i, british_keywords, climate_keywords, name_class_1 = 'Brexit', name_class_2 = 'Climate'))\n",
    "\n",
    "# headlines_df['classification'] = classification_lst\n",
    "# headlines_df.head(50)\n",
    "# headlines_df.classification.value_counts()\n",
    "\n",
    "# #Testing\n",
    "# #To do find more elegant way to deal with eu than \" eu \"\n",
    "# test_lst = []\n",
    "\n",
    "# test_headline = headlines_df.iloc[43]['headline']     \n",
    "# test_lst.append(headline_classifier(test_headline, british_keywords, climate_keywords, name_class_1 = 'Brexit', name_class_2 = 'Climate'))              \n",
    "# test_lst\n",
    "\n",
    "# test_lst\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
