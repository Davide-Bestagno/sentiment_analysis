{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up, data, dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Davide/Documents/University/RA/text_analysis/sentiment_analysis-master'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "#Set working directory \n",
    "#os.chdir('/Users/Davide/Documents/University/RA/text_analysis')\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_df = pd.read_csv('express.csv', index_col = 'date')\n",
    "bbc_df = pd.read_csv('News_dataset.csv', sep=';')\n",
    "kaggle_df = pd.read_json (r'/Users/Davide/Documents/University/RA/text_analysis/sentiment_analysis-master/News_Category_Dataset_v2.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>newspaper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Prince Andrew 'frozen out by Charles over dama...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>EXCLUSIVE: Migrants to put Britain's populatio...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>EXCLUSIVE: Jihadi John exposed by web error: K...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>Missing Becky Watts: Two arrested as family ad...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1-3-2015</th>\n",
       "      <td>We can all benefit from a positive approach to...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Man United player: I was worried after manager...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Mark Lawrenson: My biggest worry about Liverpool</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>La Liga ace reveals messages received from Man...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Real Madrid News: James Rodriguez wants to joi...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31-3-2017</th>\n",
       "      <td>Star midfielder ruled out of Merseyside derby:...</td>\n",
       "      <td>Express</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219422 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    headline newspaper\n",
       "date                                                                  \n",
       "1-3-2015   Prince Andrew 'frozen out by Charles over dama...   Express\n",
       "1-3-2015   EXCLUSIVE: Migrants to put Britain's populatio...   Express\n",
       "1-3-2015   EXCLUSIVE: Jihadi John exposed by web error: K...   Express\n",
       "1-3-2015   Missing Becky Watts: Two arrested as family ad...   Express\n",
       "1-3-2015   We can all benefit from a positive approach to...   Express\n",
       "...                                                      ...       ...\n",
       "31-3-2017  Man United player: I was worried after manager...   Express\n",
       "31-3-2017   Mark Lawrenson: My biggest worry about Liverpool   Express\n",
       "31-3-2017  La Liga ace reveals messages received from Man...   Express\n",
       "31-3-2017  Real Madrid News: James Rodriguez wants to joi...   Express\n",
       "31-3-2017  Star midfielder ruled out of Merseyside derby:...   Express\n",
       "\n",
       "[219422 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complete_Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "      <td>001.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "      <td>002.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "      <td>003.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "      <td>004.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "      <td>005.txt-business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>397.txt</td>\n",
       "      <td>BT program to beat dialler scams\\n\\nBT is intr...</td>\n",
       "      <td>tech</td>\n",
       "      <td>397.txt-tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>398.txt</td>\n",
       "      <td>Spam e-mails tempt net shoppers\\n\\nComputer us...</td>\n",
       "      <td>tech</td>\n",
       "      <td>398.txt-tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>399.txt</td>\n",
       "      <td>Be careful how you code\\n\\nA new European dire...</td>\n",
       "      <td>tech</td>\n",
       "      <td>399.txt-tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns\\n\\nThe man mak...</td>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt-tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming\\n\\nOnline rol...</td>\n",
       "      <td>tech</td>\n",
       "      <td>401.txt-tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_Name                                            Content  Category  \\\n",
       "0      001.txt  Ad sales boost Time Warner profit\\n\\nQuarterly...  business   \n",
       "1      002.txt  Dollar gains on Greenspan speech\\n\\nThe dollar...  business   \n",
       "2      003.txt  Yukos unit buyer faces loan claim\\n\\nThe owner...  business   \n",
       "3      004.txt  High fuel prices hit BA's profits\\n\\nBritish A...  business   \n",
       "4      005.txt  Pernod takeover talk lifts Domecq\\n\\nShares in...  business   \n",
       "...        ...                                                ...       ...   \n",
       "2220   397.txt  BT program to beat dialler scams\\n\\nBT is intr...      tech   \n",
       "2221   398.txt  Spam e-mails tempt net shoppers\\n\\nComputer us...      tech   \n",
       "2222   399.txt  Be careful how you code\\n\\nA new European dire...      tech   \n",
       "2223   400.txt  US cyber security chief resigns\\n\\nThe man mak...      tech   \n",
       "2224   401.txt  Losing yourself in online gaming\\n\\nOnline rol...      tech   \n",
       "\n",
       "     Complete_Filename  \n",
       "0     001.txt-business  \n",
       "1     002.txt-business  \n",
       "2     003.txt-business  \n",
       "3     004.txt-business  \n",
       "4     005.txt-business  \n",
       "...                ...  \n",
       "2220      397.txt-tech  \n",
       "2221      398.txt-tech  \n",
       "2222      399.txt-tech  \n",
       "2223      400.txt-tech  \n",
       "2224      401.txt-tech  \n",
       "\n",
       "[2225 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slice the df to have only Green and environment articles\n",
    "green_df = kaggle_df[kaggle_df.category == 'GREEN']\n",
    "env_df = kaggle_df[kaggle_df.category == 'ENVIRONMENT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Cleaning\n",
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def cleaning(df, column_str):\n",
    "    '''function to clean a text within a column of a dataframe. The column name has to be a string'''\n",
    "    #Special character\n",
    "    df[column_str] = df[column_str].str.replace(\"\\r\", \" \")\n",
    "    df[column_str] = df[column_str].str.replace(\"\\n\", \" \")\n",
    "    df[column_str] = df[column_str].str.replace(\"    \", \" \")\n",
    "    df[column_str] = df[column_str].str.replace('\"', '')\n",
    "\n",
    "    #Lowercase\n",
    "    df[column_str] = df[column_str].str.lower()\n",
    "\n",
    "    #Punctuation\n",
    "    punctuation_signs = list(\"?:!.,;-\")\n",
    "\n",
    "    for punct_sign in punctuation_signs:\n",
    "        df[column_str] = df[column_str].str.replace(punct_sign, '')\n",
    "\n",
    "    #Possessive pronouns\n",
    "    df[column_str] = df[column_str].str.replace(\"'s\", \"\")\n",
    "    df[column_str] = df[column_str].str.replace(\"'\", \"\")\n",
    "\n",
    "    #Lemmatization\n",
    "    # Saving the lemmatizer into an object\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    nrows = len(df)\n",
    "    lemmatized_text_list = []\n",
    "\n",
    "    for row in range(0, nrows):\n",
    "\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "\n",
    "        # Save the text and its words into an object\n",
    "        text = df.iloc[row][column_str]\n",
    "        text_words = text.split(\" \")\n",
    "\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "\n",
    "    df[column_str] = lemmatized_text_list\n",
    "\n",
    "    #Stopwords\n",
    "    # Downloading the stop words list\n",
    "    nltk.download('stopwords')\n",
    "    # Loading the stop words in english\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "\n",
    "    df[column_str] = df[column_str]\n",
    "\n",
    "    for stop_word in stop_words:\n",
    "\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        df[column_str] = df[column_str].str.replace(regex_stopword, '')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/Davide/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Davide/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "green_df = cleaning(green_df, 'headline')\n",
    "bbc_df = cleaning(bbc_df, 'Content')\n",
    "env_df = cleaning(env_df, 'headline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df = bbc_df[bbc_df.Category == 'business']\n",
    "pol_df = bbc_df[bbc_df.Category == 'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most commonly used words extractor\n",
    "def keywords_extractor(df, headline_str, n):\n",
    "    '''Return the most commonly used words from a dataframe with a column containing text.\n",
    "    Arguments are dataframe, the name of the column in string format, and the number n of keywords needed'''\n",
    "    df_words = Counter()\n",
    "    df[headline_str].str.split().apply(df_words.update)\n",
    "    df_words = df_words.most_common(20)\n",
    "    df_words_lst = []\n",
    "    for tup in df_words:\n",
    "        df_words_lst.append(tup[0])\n",
    "    return df_words_lst\n",
    "\n",
    "#Keywords list creation\n",
    "biz_keywords = keywords_extractor(biz_df, 'Content', n = 20)\n",
    "pol_keywords = keywords_extractor(pol_df, 'Content', n = 20)\n",
    "green_keywords = keywords_extractor(green_df, 'headline', n = 20)\n",
    "env_keywords = keywords_extractor(env_df, 'headline', n = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'us', 'year', 'company', 'mr', 'firm', 'market', 'would', 'bank', 'rise', 'also', 'new', 'price', 'share', 'growth', 'last', 'economy', 'make', 'government', 'sales']\n"
     ]
    }
   ],
   "source": [
    "print(biz_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'mr', 'would', 'labour', 'government', 'party', 'people', 'blair', 'election', 'minister', 'plan', 'make', 'also', 'new', 'tell', 'could', 'brown', 'go', 'tax', 'lord']\n"
     ]
    }
   ],
   "source": [
    "print(pol_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['save', 'picture', 'find', 'world', 'trump', 'make', 'extreme', 'animal', 'weather', 'say', 'take', 'water', 'week', 'dog', 'climate', 'watch', 'baby', 'new', 'kill', 'photos', '2012', 'oil', 'get', 'us', 'green', 'change', 'hurricane', 'day', 'energy', 'california', 'could']\n"
     ]
    }
   ],
   "source": [
    "#Combining green and env keywords\n",
    "climate_keywords = []\n",
    "climate_keywords.extend(green_keywords)\n",
    "climate_keywords.extend(env_keywords)\n",
    "\n",
    "#Eliminating duplicates\n",
    "climate_keywords = list(set(climate_keywords))\n",
    "\n",
    "#Removing not useful keywords\n",
    "climate_keywords.remove('(photos)')\n",
    "climate_keywords.remove('(video)')\n",
    "\n",
    "print(climate_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "british_keywords = [\"brexit\", \"european union\", \" eu \", \"british\", \"british identity\", \"british passport\", \"british culture\", \"british heritage\", \"british goods\", \"british products\",\"british manufacturing\", \"made in britain\"]\n",
    "climate_keywords = [\"climate change\", \"paris agreement\", \"environment\", \"global warming\", \"unfccc\"]\n",
    "immigration_keywords = [\"\"]\n",
    "\n",
    "headlines_df['classification'] = ''\n",
    "classification_lst = []\n",
    "\n",
    "\n",
    "def headline_classifier(string, keyword_lst_class_1, keyword_lst_class_2, name_class_1 = 'class_1', name_class_2 = 'class_2'):\n",
    "    '''Classifies a list of strings according to keyword lists for class_1 and class_2'''\n",
    "    string = string.lower()\n",
    "    count_1 = 0\n",
    "    count_2 = 0\n",
    "    for keyword in keyword_lst_class_1:\n",
    "        if keyword in string:\n",
    "            count_1 =+ 1\n",
    "    for keyword in keyword_lst_class_2:\n",
    "\n",
    "        if keyword in string:\n",
    "            count_2 =+ 1\n",
    "    \n",
    "    if count_1 > 0 and count_2 > 0:\n",
    "        return 'both'\n",
    "    elif count_1 > 0:\n",
    "        return name_class_1\n",
    "    elif count_2 > 0:\n",
    "        return name_class_2\n",
    "    else:\n",
    "        return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row, n_col = headlines_df.shape\n",
    "headlines_lst = list(headlines_df['headline'].values)\n",
    "\n",
    "#Headline classification\n",
    "classification_lst = []\n",
    "for i in range(n_row):\n",
    "    headline_i = str(headlines_lst[i]) #Include this becasue of potential nan values\n",
    "    classification_lst.append(headline_classifier(headline_i, british_keywords, climate_keywords, name_class_1 = 'Brexit', name_class_2 = 'Climate'))\n",
    "\n",
    "headlines_df['classification'] = classification_lst\n",
    "headlines_df.head(50)\n",
    "headlines_df.classification.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "#To do find more elegant way to deal with eu than \" eu \"\n",
    "test_lst = []\n",
    "\n",
    "test_headline = headlines_df.iloc[43]['headline']     \n",
    "test_lst.append(headline_classifier(test_headline, british_keywords, climate_keywords, name_class_1 = 'Brexit', name_class_2 = 'Climate'))              \n",
    "test_lst\n",
    "\n",
    "test_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty list to add the polarity score\n",
    "polarity_lst = []\n",
    "subjectivity_lst = []\n",
    "\n",
    "#Headline sentiment\n",
    "for i in range(n_row):\n",
    "    headline_i = str(headlines_lst[i]) #Include this becasue of potential nan values\n",
    "    blob_headline_i = TextBlob(headline_i) #transforming string into textblob\n",
    "    polarity_lst.append(blob_headline_i.sentiment.polarity)\n",
    "    subjectivity_lst.append(blob_headline_i.sentiment.subjectivity)\n",
    "\n",
    "#Adding polarity and subjectivity scores to the headlines dataframe\n",
    "headlines_df['polarity'] = polarity_lst\n",
    "headlines_df['subjectivity'] = subjectivity_lst\n",
    "headlines_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for each class\n",
    "brexit_df = headlines_df[headlines_df.classification == 'Brexit']\n",
    "climate_df = headlines_df[headlines_df.classification == 'Climate']\n",
    "both_df = headlines_df[headlines_df.classification == 'both']\n",
    "\n",
    "#\n",
    "results_df = pd.DataFrame(index = ['polarity', 'subjectivity'], columns = ['Brexit', 'Climate', 'Both'])\n",
    "results_df['Brexit'] = brexit_df.mean(axis = 0, numeric_only = True)\n",
    "results_df['Climate'] = climate_df.mean(axis = 0, numeric_only = True)\n",
    "results_df['Both'] = both_df.mean(axis = 0, numeric_only = True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
